* need to measure request per seconds
   - total request connected in a second
   - total response sent in second
   - total request sent & same is processed in that second

   both average per each iteration & overall iteration

* need to add millisecond representaion of delay along with seconds in "all_data"
* need to add status code in perecentage along with actual count in "all_data"
* change "all_data" key to "overall metric"
*  Bandwidth (kilo bytes in/out per second)
   1. total request payload sent in size
   2. total response payload recived in size
*  prepare payload ahead of time [ optional / but important ] 
   the "payload_generator_callback" generator should be called & store the payloads before start sending request
   so any computational intensive task will not effect bench mark metrics
   & measuring Bandwidth will also cause less effect/error on metrics

* try, to create benchmark runner client & to make a decentralized client, so we can achive unique IP address
benchmark server should distribute/assign size & time of API call(could be real time or planned) to the benchmark runner client
& benchmark client should be scalable in a flexible way
i think testing can be done using docker,
& can be released as docker image and the code will be mounted to host drive, so user can write api calls,payload etc